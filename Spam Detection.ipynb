{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get spam and ham data from the spamassassin public corpus. These will be extracted separately into the spam folder in datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
    "HAM_URL = DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\n",
    "SPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\"\n",
    "SPAM_PATH = os.path.join(\"datasets\", \"spam\")\n",
    "\n",
    "def fetch_spam_data(spam_url=SPAM_URL, spam_path=SPAM_PATH):\n",
    "    if not os.path.isdir(spam_path):\n",
    "        os.makedirs(spam_path)\n",
    "    for filename, url in ((\"ham.tar.bz2\", HAM_URL), (\"spam.tar.bz2\", SPAM_URL)):\n",
    "        path = os.path.join(spam_path, filename)\n",
    "        if not os.path.isfile(path):\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "        tar_bz2_file = tarfile.open(path)\n",
    "        tar_bz2_file.extractall(path=SPAM_PATH)\n",
    "        tar_bz2_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide different directory addresses which will be passed onto the reader function. Two lists are created to iterate through all the filenames after extracting the tarball file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAM_DIR = os.path.join(SPAM_PATH, \"easy_ham\")\n",
    "SPAM_DIR = os.path.join(SPAM_PATH, \"spam\")\n",
    "ham_filenames = [name for name in sorted(os.listdir(HAM_DIR)) if len(name) > 20]\n",
    "spam_filenames = [name for name in sorted(os.listdir(SPAM_DIR)) if len(name) > 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "Given a file name we can open it using the open method. There are some invalid characters that we need to strip out which can be done using a sequence of regular expressions. Any string of characters delimited by a space that has a non-alphabetical letter is removed from the e-mail. The read_email function returns a Counter object of what words occur in the e-mail and to what frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "regex_rules = {\n",
    "    'tags' : '<[^>]*>',\n",
    "    'brackets': '\\([^\\)]*\\)',\n",
    "    'newline': '(\\n+)'\n",
    "}\n",
    "\n",
    "# Get the words of an email given a filename\n",
    "def read_email(fname, spam = 0):\n",
    "    url = os.path.join(SPAM_DIR if spam else HAM_DIR, fname)\n",
    "    \n",
    "    with open(url, \"rb\") as f:\n",
    "        try:\n",
    "            chunk = str(f.read())\n",
    "            for k, v in regex_rules.items():\n",
    "                chunk = re.sub(v, '', chunk)\n",
    "        except UnicodeDecodeError:\n",
    "            pass\n",
    "        \n",
    "    chunk_to_words = chunk.split(' ')\n",
    "    words = set([word.lower() for word in chunk_to_words if re.match('^[a-zA-Z]+$', word) and len(word) > 2])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run this function on the spam and ham files and label them appropriately into spam (1) and ham (0). First iterate through all the counter objects to see what words occur most frequently across the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_words = [(read_email(file_name), 0) for file_name in ham_filenames]\n",
    "spam_words = [(read_email(file_name,1), 1) for file_name in spam_filenames]\n",
    "spam_data = ham_words + spam_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = list(map(lambda x: x[0], spam_data))\n",
    "y = list(map(lambda x: x[1], spam_data))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We establish the proportion of documents that contain a specific word are classified as spam/not spam. This will be used in the framework of Naive Bayes; picking out documents containing words that feature more in spam documents than ham documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def count_words(documents):\n",
    "    counts = defaultdict(lambda: [0,0])\n",
    "    for i, document in enumerate(documents):\n",
    "        for word in document:\n",
    "            counts[word][1 if y_train[i] else 0] += 1 # Add 1 to spam counter if the y label is spam otherwise ham counter\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure how frequent each word appears in a spam/ham document and report it out. An added k term is done to make probabilities non-zero, avoiding cases where words that don't appear in ham end up always being classified as spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smoothing k term for words that appear on only one side of spam/ham\n",
    "# so that a new email that has a word that features only in the ham training set does not always get classified as ham \n",
    "def word_probabilities(counts, total_spams, total_non_spams, k=0.5):\n",
    "    return [(w,\n",
    "            (spam + k)/ (total_spams + 2 * k),\n",
    "            (non_spam + k) / (total_non_spams + 2 * k))\n",
    "            for w, (spam, non_spam) in counts.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def spam_probability(word_probs, message):\n",
    "    log_prob_if_spam = log_prob_if_not_spam = 0.0\n",
    "    \n",
    "    for word, prob_if_spam, prob_if_not_spam in word_probs:\n",
    "        # if word appears in message add the log probability of seeing it\n",
    "        if word in message:\n",
    "            log_prob_if_spam += math.log(prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(prob_if_not_spam)\n",
    "        # if the word doesn't appear in message add the log probability of not seeing it\n",
    "        else:\n",
    "            log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "            log_prob_if_not_spam += math.log(1.0 - prob_if_not_spam)\n",
    "            \n",
    "    prob_if_spam = math.exp(log_prob_if_spam)\n",
    "    prob_if_not_spam = math.exp(log_prob_if_not_spam)\n",
    "    \n",
    "    return prob_if_spam / (prob_if_spam + prob_if_not_spam)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "405 1995 2400\n"
     ]
    }
   ],
   "source": [
    "word_frequencies = count_words(X_train)\n",
    "num_spams = len([message for i, message in enumerate(X_train) if y_train[i]])\n",
    "num_non_spams = len(X_train) - num_spams\n",
    "word_prob = word_probabilities(word_frequencies, num_spams, num_non_spams, k=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the NLTK library we can strip out words from the Counters that are strictly nouns or not words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import nltk\n",
    "\n",
    "is_noun = lambda pos: pos[:2] == 'NN'\n",
    "\n",
    "class SelectNouns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X = [list(X[i]) for i, _ in enumerate(X)]\n",
    "        print(X[0])\n",
    "        tags = [nltk.pos_tag(document) for document in X]\n",
    "        nouns = [[noun[0] for noun in document if is_noun(noun[1])] for document in tags]\n",
    "        return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sept', 'localhost', 'national', 'khare', 'esmtp', 'rohit', 'murcko', 'imap', 'message', 'with', 'from', 'jalapeno', 'for', 'the', 'sep', 'friends', 'thu']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "noun_transformer = SelectNouns()\n",
    "spam_data_prepared = noun_transformer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, k = 0.5):\n",
    "        self.k = k\n",
    "        self.word_probs = []\n",
    "        \n",
    "    def train(self, training_set):\n",
    "        num_spams = len([message for i, message in enumerate(training_set) if y_train[i]])\n",
    "        num_non_spams = len([message for i, message in enumerate(training_set) if not y_train[i]])\n",
    "        word_counts = count_words(training_set)\n",
    "        self.word_probs = word_probabilities(word_counts,\n",
    "                                            num_spams,\n",
    "                                            num_non_spams,\n",
    "                                            self.k)\n",
    "    def classify(self,message):\n",
    "        return spam_probability(self.word_probs, message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-ed60b209b35a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnb_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnb_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-42-df1eb037dbe4>\u001b[0m in \u001b[0;36mclassify\u001b[1;34m(self, message)\u001b[0m\n\u001b[0;32m     13\u001b[0m                                             self.k)\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mspam_probability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-39-bc2a8684407c>\u001b[0m in \u001b[0;36mspam_probability\u001b[1;34m(word_probs, message)\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[1;31m# if the word doesn't appear in message add the log probability of not seeing it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m             \u001b[0mlog_prob_if_spam\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mprob_if_spam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m             \u001b[0mlog_prob_if_not_spam\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mprob_if_not_spam\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "nb_clf = NaiveBayesClassifier()\n",
    "\n",
    "nb_clf.train(X_train)\n",
    "nb_clf.classify(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
